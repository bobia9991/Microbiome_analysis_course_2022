---
title: " Microbial Data Analysis Course"
output: html_document 
date: "15-03-2022"
---

This is the schedule for the practical session:

- Beta diversity calculation (between sample diversity) & visualization: 40 minutes
- Differential abundance analysis: 40 minutes
- Break: 10 minutes

## Loading packages and data in R

We first load the packages that we will employ during the tutorial

```{r, message=FALSE}
library(knitr)
library(tidyverse)
library(reshape2)
library(dplyr)
library(plyr)
library(ggrepel)
library(ggplot2)
library(ggpubr)
library(pROC)
library(car)
library(vegan)
library(SIAMCAT)

set.seed(540832)
```

Next, we read the data tables that contain the count matrix and samples' metadata. 
```{r, message=FALSE}
# set the working directory
setwd("~/Documents/SAEZ/teaching/Microbiome_analysis_course_2022/")
load("data/mobi.Rdata")
```

################################################################################
# Differential Abundance Analysis
################################################################################

# Normalization

There are multiple factors that can result in libraries of different sizes. 
Those include experimental variations, batch effects or simply, different sequencing depths. 
We assume that, if it were not for these variations, all samples should have a similar 
range and distribution of abundance Therefore, after data filtering, a normalization step 
is necessary to ensure that species abundance can be compared between samples and experimental 
conditions. Below, we use the relative abundance. There are other normalisation approaches described 
in [Pereira et al. 2018](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4637-6).

```{r}
# apply normalization function
motu.abs.rel <- prop.table(as.matrix(motu.abs), 2)
motu.abs.fil.rel <- prop.table(as.matrix(motu.abs.fil), 2)
```

And have a look to how the normalized values look like:

```{r}
head(motu.abs.fil.rel)
```


# Differential Expression Analysis

Next, we can analyze the differences between the transcriptomic profiles of both cell lines. To do so, we carry out what is known as Differential Expression Analysis (DEA). This is a statistical analysis where all species in the count matrix are compared between the sample groups of interest. To perform DEA, we use the `limma` package. `limma` was originally released as a software package to perform the statistical analysis of microarray data, but was later [adapted for the analysis of RNA-Seq](https://academic.oup.com/nar/article/43/7/e47/2414268). `limma` fits a linear model for each gene according to the experimental design to test the null hypothesis that no gene is differentially abundant between experimental conditions, and calculates a moderated *t*-statistic. To do so, it uses an empirical Bayes method to improve estimation of the underlying variance. For more information, please see "Shared global parameters link gene-wise models" and "Empirical Bayes borrows information between species" of the aforementioned paper. `limma` is not the only option to perform DEA. Common alternatives include [`edgeR`](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2796818/) and [`DESeq2`](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0550-8). The strategy that we employ here is known as limma-trend, where the initial count matrix is normalized using `edgeR` but the DEA is carried out with `limma`. For more information please see [Limma's users guide](https://www.bioconductor.org/packages/devel/bioc/vignettes/limma/inst/doc/usersguide.pdf).


And look at the distribution of the adjusted P values:

```{r}
ggplot2::ggplot(de_table, aes(x = adj.P.Val)) +
  ggplot2::geom_histogram()
```

**NOTE:** Do you remember how the P-value distribution should look like? If not, please see [here](http://varianceexplained.org/statistics/interpreting-pvalue-histogram/).

Our P-value histogram clearly shows an anti-conservative distribution. We can also visualize the gene expression changes using one of the most common plots to explore DE results, the **volcano plot**:

```{r}
# # set cutoffsa
# p_cutoff <- 0.0001
# fc_cutoff <- 2
featTable=motu.abs.fil.rel
metaTable=meta
  # prepare matrix
  featTable <- featTable[!rownames(featTable)=="-1", , drop = FALSE]
  
  df = featTable[, rownames(metaTable)]
  metaTable$ID = rownames(metaTable)
  p.val <- matrix(NA, nrow=nrow(featTable), ncol=1, 
                  dimnames=list(row.names(featTable)))
  fc <- p.val
  aucs.mat <- p.val
  aucs.all  <- vector('list', nrow(featTable))
  ##############################################################################
  # calculate wilcoxon test and effect size for each feature
  p.cal <- tribble(~ID, ~pval,~adj.pval,~aucs.mat, ~fc, ~log10, ~sig )
  
  for (rowname in row.names(featTable)) {
    
    x <- as.numeric(featTable[rowname, metaTable %>% filter(status=='PC') %>% pull(ID)])
    y <- as.numeric(featTable[rowname, metaTable %>% filter(status=='CTR') %>% pull(ID)])
    
    # Wilcoxon
    #p.val <- wilcox.test(x, y, exact=FALSE)$p.value

    # AUC
    # aucs.all  <- roc(controls=y, cases=x, 
    #                              direction='<', ci=TRUE, auc=TRUE)$ci
    # aucs.mat <- roc(controls=y, cases=x, 
    #                        direction='<', ci=TRUE, auc=TRUE)$ci[2]
    # 
    # FC
    q.p <- quantile(log10(x+log.n0), probs=seq(.1, .9, .05))
    q.n <- quantile(log10(y+log.n0), probs=seq(.1, .9, .05))
    #fc <- sum(q.p - q.n)/length(q.p)
    
    p.cal=add_row(p.cal, 
                  ID=rowname,
                  pval= wilcox.test(x, y, exact=FALSE)$p.value,
                  aucs.mat=roc(controls=y, cases=x, direction='<', ci=TRUE, auc=TRUE)$ci[2], 
                  fc=sum(q.p - q.n)/length(q.p),
                  adj.pval=p.adjust(pval))
                   # log10=-log10(as.numeric((adj.pval) )))
  }
  

   for (f in row.names(featTable)) {
    
    x <- as.numeric(featTable[f, metaTable %>% filter(status=='PC') %>% pull(ID)])
    y <- as.numeric(featTable[f, metaTable %>% filter(status=='CTR') %>% pull(ID)])
    
    # Wilcoxon
    p.val[f,1] <- wilcox.test(x, y, exact=FALSE)$p.value

    # AUC
    aucs.all[[f]][[1]]  <- c(roc(controls=y, cases=x, 
                                 direction='<', ci=TRUE, auc=TRUE)$ci)
    aucs.mat[f,1] <- c(roc(controls=y, cases=x, 
                           direction='<', ci=TRUE, auc=TRUE)$ci)[2]
    
    # FC
    q.p <- quantile(log10(x+log.n0), probs=seq(.1, .9, .05))
    q.n <- quantile(log10(y+log.n0), probs=seq(.1, .9, .05))
    fc[f,1] <- sum(q.p - q.n)/length(q.p)
    
  }
  
  
   ##############################################################################
  # fdr correction
  p.adj <- data.frame(apply(p.val, MARGIN=2, FUN=p.adjust, method="fdr"),
                      check.names = FALSE)
  colnames(p.adj) <- "adj"
  
  # add fc and auc
  p.adj$p.val <- p.val
  p.adj$fc <- fc
  p.adj$auc.mat <- aucs.mat
  p.adj$log10p = -log10(as.numeric(p.adj$adj))
  p.adj <- as.data.frame(p.adj)
  rownames(p.adj) <- make.names(rownames(df), unique=TRUE)
  #print(head(p.adj))
  p.adj <- p.adj %>% na.omit(p.adj)
  p.adj$significant <- ifelse(p.adj$adj < 0.05, "p.adj < 0.05", "not sig")
  p.adj$species <-rownames(p.adj)
  # save file
  write.table(p.adj, file=paste0(PARAM$folder.results, 'p.adj.tsv'), 
              sep='\t', quote=FALSE, row.names=TRUE, col.names=TRUE)
  
  


#  volcano plots
ggplot2::ggplot(de_table, aes(x = logFC,-log10(adj.P.Val), color = status)) +
  ggplot2::geom_point() +
  ggplot2::geom_vline(xintercept = log2(c(0.25, 0.5, 2, 4)), lty = 2) +
  ggplot2::geom_hline(yintercept = -log10(c(0.05, 0.01, 0.001, 0.0001)), lty = 2) +
  ggplot2::scale_color_manual(values = c(
    "Up" = "red",
    "Down" = "blue",
    "Other" = "black"
  ))


# Volcano plot significant features
plot.volcano <- function(p.adj.df, plot.name){
  p <- ggplot(p.adj.df, aes(x = sc.fc, y = log10p)) +
    geom_point(aes(size=rel, color = significant), alpha=0.5) +
    scale_color_manual(values = c("black","red")) +
    theme_bw(base_size = 20) +
    geom_hline(yintercept=-log10(0.05))+
    geom_text_repel(data = subset(p.adj.df, adj < 0.05),
                    aes(label = species),
                    size = 3,
                    box.padding = unit(0.2, "lines"),
                    point.padding = unit(0.2, "lines")) +
    ggtitle("Differentially abundant species") +
    xlab("Generalized fold change") + 
    ylab("Log10 adjusted p-value") +
    theme_classic()
  
  print(p)
  # save plot
  ggsave(p, filename=paste0(PARAM$folder.results, plot.name, "wilcox.volcano.pdf"), 
         width = 5.5, height=6)
} 
```

A volcano plot enables us to quickly visualize the magnitude (logFC) and significance (-log10(pvalue)) of DE changes. Each point represent a gene, and its color indicates whether they surpass or not a cutoff of an absolute logFC > `r fc_cutoff` and an adjusted P value < `r p_cutoff`. 

**QUESTION**: What do the horizontal and vertical dashed lines mean in this plot? Take a look at the code and think about their meaning. 

**QUESTION**: Try to modify the `fc_cutoff` and and `p_cutoff` variables in this chunk. What happens? What cutoff values should we use? 

# Functional analysis

Thanks to the DEA, we know which species are different between the two cell lines, with measurements that indicate us the magnitude and the significance of the changes. But now we face one of the most prominent bottlenecks in the analysis of omics data: The extraction of biological insights that we can further interpret and link to a particular phenotype. 
```{r}
meta.train <- meta
feat.train <- feat.all
  
siamcat <- siamcat(feat=feat.train, meta=meta.train,
                    label = 'status', case='PC')
siamcat <- filter.features(siamcat, filter.method = 'abundance', 
                             cutoff=0.001, verbose=3)
siamcat <- normalize.features(siamcat, norm.method = norm.method,
                                norm.param = n.p, feature.type = 'filtered',
                                verbose=3)
siamcat <- create.data.split(siamcat, num.folds = num.folds,
                               num.resample = num.resample)
siamcat <- train.model(siamcat,
                         method = model
                        )
siamcat <- make.predictions(siamcat)
siamcat <- evaluate.predictions(siamcat)
models[[model]] <- siamcat
save(siamcat, file=paste0(PARAM$folder.R, 'nonfiltered.RData'))


siamcat <- siamcat(feat=featTable, meta=metaTable, label="subject_disease_status", case=case)
  
  # filter based on abundance 
  siamcat <- filter.features(siamcat, filter.method = 'abundance', 
                             cutoff=0.001, verbose=3)
  check.confounders(siamcat, fn.plot = paste0(PARAM$folder.results, 
                                              fileName, 'confounders.pdf'),
                    meta.in=metatest, verbose = 3)
  
  # normalize with log.clr
  siamcat <- normalize.features(siamcat, norm.method = "log.clr", feature.type = 'filtered',
                                norm.param = list(log.n0=1e-05, sd.min.q=1))
  
  # compute associations 
  siamcat <- check.associations(siamcat, feature.type = 'normalized', 
                                detect.lim = 10^-5, plot.type = "quantile.box",
                                fn.plot = paste0(PARAM$folder.results, 
                                                 Sys.Date(), '.', fileName,'assoc.plot.pdf'))
  
  # train model
  siamcat <- create.data.split(siamcat, num.folds =10, num.resample = 10)  
  siamcat <- train.model(siamcat, method = "lasso_ll", verbose = 2)
  siamcat <- make.predictions(siamcat)
  siamcat <- evaluate.predictions(siamcat)    
  print(siamcat@eval_data$auroc)
  # evaluation plot
  model.evaluation.plot(siamcat, fn.plot = paste0(PARAM$folder.results, Sys.Date(), '.',
                                                  fileName, 'eval.plot.pdf'))
  # interpretation plot
  model.interpretation.plot(siamcat, fn.plot = paste0(PARAM$folder.results, 
                                                      Sys.Date(), '.', fileName,'interpret.plot.pdf'),
                            consens.thres = 0.5,
                            detect.lim = 1e-05,
                            heatmap.type = 'zscore')
  
  # save siamcat object
  save(siamcat, file = paste0(PARAM$folder.results, fileName, 'siamcat.Rdata'))
```

```{r}
# add confounders to model
add.meta <- function(x, n){
  x <- add.meta.pred(x, pred.names = n, verbose=3)
  x <- train.model(x, method='lasso_ll', verbose=2, perform.fs = TRUE,
                   param.fs = list(thres.fs=50, method.fs='gFC', 
                                   direction='positive'))
  x <- make.predictions(x)
  x <- evaluate.predictions(x)
  return(x)
}
# combine with naive model
siamcat.jau <- add.meta(siamcat.pc, 'jaundice')
```

